{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'c:\\\\tensorflow1\\\\models\\\\research\\\\object_detection'"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'c:/tensorflow1/models/research/object_detection')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python export_tflite_ssd_graph.py \\\n",
    "        --pipeline_config_path=training/ssd_mobilenet_v2.config \\\n",
    "        --trained_checkpoint_prefix=training/ssd_mobilenet/model.ckpt-9105 \\\n",
    "        --output_directory=tflite \\\n",
    "        --add_postprocessing_op=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_model = \"./tflite/tflite_graph.pb\"\n",
    "out_model = \"./tflite/tflite_gprah.tflite\"\n",
    "\n",
    "# check input_arrays and output_arrays of model\n",
    "gf = tf.GraphDef()   \n",
    "m_file = open(in_model,'rb')\n",
    "gf.ParseFromString(m_file.read())\n",
    "\n",
    "with open('./tflite/graph.txt', 'a') as the_file:\n",
    "    for n in gf.node:\n",
    "        the_file.write(n.name+'\\n')\n",
    "\n",
    "file = open('./tflite/graph.txt','r')\n",
    "data = file.readlines()\n",
    "print (\"output name = \")\n",
    "print (data[len(data)-1])\n",
    "\n",
    "print (\"Input name = \")\n",
    "file.seek ( 0 )\n",
    "print (file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arrays = [\"normalized_input_image_tensor\"]\n",
    "output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1',\n",
    "                 'TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\n",
    "input_shapes = {\"normalized_input_image_tensor\":[1,300,300,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_frozen_graph(in_model, input_arrays,\n",
    "                                                      output_arrays, input_shapes)\n",
    "#converter.post_training_quantize = True\n",
    "converter.allow_custom_ops = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(out_model, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'c:\\\\tensorflow1\\\\models\\\\research\\\\object_detection'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import importlib.util\n",
    "import time\n",
    "\n",
    "os.chdir(r'c:/tensorflow1/models/research/object_detection')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum confidence threshold for displaying detected objects\n",
    "min_conf_threshold = float(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to .tflite file, which contains the model that is used for object detection\n",
    "PATH_TO_CKPT = 'tflite/ssd_mobilenet_q/tflite_graph.tflite'\n",
    "\n",
    "# Path to label map file\n",
    "PATH_TO_LABELS = 'tflite/labelmap.txt'\n",
    "# Load the label map\n",
    "with open(PATH_TO_LABELS, 'r') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Have to do a weird fix for label map if using the COCO \"starter model\" from\n",
    "# https://www.tensorflow.org/lite/models/object_detection/overview\n",
    "# First label is '???', which has to be removed.\n",
    "if labels[0] == '???':\n",
    "    del(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['thistle']"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tensorflow Lite model.\n",
    "# If using Edge TPU, use special load_delegate argument\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "interpreter = Interpreter(model_path=PATH_TO_CKPT)\n",
    "\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "floating_model = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "input_mean = 127.5\n",
    "input_std = 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "floating_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['DJI_0032.JPG',\n 'DJI_0033.JPG',\n 'DJI_0034.JPG',\n 'DJI_0036.JPG',\n 'DJI_0037.JPG',\n 'DJI_0038.JPG',\n 'DJI_0039.JPG',\n 'DJI_0040.JPG',\n 'DJI_0041.JPG',\n 'DJI_0042.JPG',\n 'weed.jpg',\n 'weeds.jpg']"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "# images to perform detection on\n",
    "images = 'test/'\n",
    "os.listdir(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency = list()\n",
    "# Loop over every image and perform detection\n",
    "for img in os.listdir(images):\n",
    "    start = time.clock()\n",
    "    image_path = images + img\n",
    "    # Load image and resize to expected shape [1xHxWx3]\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (640, 480), interpolation=cv2.INTER_LINEAR)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    imH, imW, _ = image.shape \n",
    "    image_resized = cv2.resize(image_rgb, (width, height))\n",
    "    input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "    if floating_model:\n",
    "        input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "    # Perform the actual detection by running the model with the image as input\n",
    "    interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Retrieve detection results\n",
    "    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
    "    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n",
    "    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\n",
    "    #num = interpreter.get_tensor(output_details[3]['index'])[0]  # Total number of detected objects (inaccurate and not needed)\n",
    "\n",
    "    # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "    for i in range(len(scores)):\n",
    "        if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\n",
    "\n",
    "            # Get bounding box coordinates and draw box\n",
    "            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "            ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "            xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "            ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "            xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "            \n",
    "            cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "\n",
    "            # Draw label\n",
    "            #object_name = labels[0] \n",
    "            object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
    "            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "            # Draw white box to put label text in\n",
    "            cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) \n",
    "            cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "    end = time.clock()\n",
    "    latency.append(end - start)\n",
    "    # All the results have been drawn on the image, now display the image\n",
    "    cv2.imshow('Object detector', image)\n",
    "\n",
    "    # Press any key to continue to next image, or press 'q' to quit\n",
    "    if cv2.waitKey(0) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.9976064 , 0.987175  , 0.12670368, 0.12670368, 0.12670368,\n       0.05341607, 0.0523698 , 0.04061228, 0.02609569, 0.02609569],\n      dtype=float32)"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "average latency is:  0.9892215583333458\nlatency per image is:  [0.990690000000086, 1.001221600000008, 1.0038567000001422, 1.0172973999999613, 1.0194687999999132, 0.9990970999999718, 0.9912228000000596, 1.0103646000000026, 1.0030853000000661, 1.0219426999999541, 0.802298300000075, 1.0101133999999092]\n"
    }
   ],
   "source": [
    "print('average latency is: ', np.mean(latency))\n",
    "print('latency per image is: ', latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "latency = 0\n",
    "fps = 0\n",
    "\n",
    "# Open video file\n",
    "video = cv2.VideoCapture('weed.mp4')\n",
    "#video.set(CV_CAP_PROP_FOURCC, CV_FOURCC('A', 'V', 'C', '1'))\n",
    "imW = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "imH = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "while(video.isOpened()):\n",
    "    start = time.clock()\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        pass\n",
    "    # resize to expected shape [1xHxWx3]\n",
    "    frame = cv2.resize(frame, (640, 480), interpolation=cv2.INTER_LINEAR)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_resized = cv2.resize(frame_rgb, (width, height))\n",
    "    input_data = np.expand_dims(frame_resized, axis=0)\n",
    "\n",
    "    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "    if floating_model:\n",
    "        input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "    # Perform the actual detection by running the model with the image as input\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Retrieve detection results\n",
    "    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
    "    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n",
    "    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\n",
    "    #num = interpreter.get_tensor(output_details[3]['index'])[0]  # Total number of detected objects (inaccurate and not needed)\n",
    "\n",
    "    # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "    for i in range(len(scores)):\n",
    "        if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\n",
    "\n",
    "            # Get bounding box coordinates and draw box\n",
    "            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "            ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "            xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "            ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "            xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "            \n",
    "            cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), (10, 255, 0), 4)\n",
    "\n",
    "            # Draw label\n",
    "            #object_name = labels[0] \n",
    "            object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
    "            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "            # Draw white box to put label text in\n",
    "            cv2.rectangle(frame, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) \n",
    "            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "    end = time.clock()\n",
    "    i = i + 1\n",
    "    latency = latency + (end - start)\n",
    "    fps = fps + 1.0 / (end - start)\n",
    "    # All the results have been drawn on the image, now display the image\n",
    "    cv2.imshow('Object detector', frame)\n",
    "\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "average latency is:  6.7429033800000635\naverage fps is: 109.68058377653487\n"
    }
   ],
   "source": [
    "print('average latency is: ', (latency / i))\n",
    "print('average fps is:', (fps / i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37464bitmlconda0481b47325a04b0ba28ffb2753b34d40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}